{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Iteration 1 for Movie Sentiment Analysis </h1>\n",
    "<h2> Yuzhong Huang & Wilson Tang </h2>\n",
    "<h3> Data Description\n",
    "</h3>\n",
    "<p>\n",
    "\"There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.\"\n",
    "The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee. In their work on sentiment treebanks, Socher et al. used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Exploration Journal Debrief\n",
    "</h1>\n",
    "<p>\n",
    "We first followed the bag of words kaggle tutorial(https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words) to create a (256060, 1000) bag of words as a feature map, and applied random forest classification to fit our trainning data. Due to limitedness of memory, we were only able to perform random forest classification at tree level of 10. And we achieved score of 0.56750 in Kaggle.\n",
    "We then used scikit's naive bayesian model and explored using \n",
    "    - tfidf \n",
    "    - simple frequencies \n",
    "    - our own model(started)\n",
    "Naive Bayesian appeared to work better and we will be further working on improving it later before moving onto more techniques\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuzhong/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/yuzhong/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yuzhong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get train & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"train.tsv\", sep='\\t')\n",
    "test = pd.read_csv(\"test.tsv\", sep='\\t')\n",
    "\n",
    "# drop nan data in train\n",
    "train = train.dropna()\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\" This function will take in a loaded review as well as a list of stop words and process it by\n",
    "    removing non letters, converting to lowercase and splitting, removing stopwords, and \n",
    "    returning a reconstructed review\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return ( \" \".join(meaningful_words ))\n",
    "\n",
    "def add_processed_review(df):\n",
    "    \"\"\" add_processed_review takes a dataframe and process the phrases within the dataframe and \n",
    "    add a new column \"ProcessedReview\".\n",
    "    \"\"\"\n",
    "    df['ProcessedReview'] = df['Phrase'].apply(review_to_words)\n",
    "    return df\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "def get_word_bag(vect, data):\n",
    "    \"\"\"\n",
    "    Takes a list of text data and a vectorizer to transform the data to a bag of words in list\n",
    "    \"\"\"\n",
    "    bag = vect.fit_transform(data)\n",
    "    return bag.toarray()\n",
    "\n",
    "def nb_model(train_features, test_features, target):\n",
    "    \"\"\"\n",
    "    Return the predictions from navie baysian trained from given features and target\n",
    "    \"\"\"\n",
    "    nb = MultinomialNB().fit(train_features, target)\n",
    "    return nb.predict(test_features)\n",
    "\n",
    "def submission(predictions):\n",
    "    \"\"\"\n",
    "    Use pandas to write the comma-separated output \n",
    "    file based on the predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "            \"PhraseId\": test[\"PhraseId\"],\n",
    "            \"Sentiment\": predictions\n",
    "        })\n",
    "\n",
    "    submission.to_csv(\"Movie_Sentiment_Analysis_Submission.csv\", index=False, quoting=3)\n",
    "\n",
    "###################################################################################################\n",
    "    \n",
    "def add_whole_sentence(df):\n",
    "    \"\"\"\n",
    "    Generates a boolean 'WholeSentence' column for given dataset\n",
    "    to indicate whether the phrase is a whole sentence or not\n",
    "    \"\"\"\n",
    "    df['WholeSentence'] = False\n",
    "    # initialize a last_id for comparison\n",
    "    last_id = 0\n",
    "    # loop throught the dataframe to set the first sentence for each sentence group's 'WholeSentence' to be True\n",
    "    for index, row in df.iterrows():\n",
    "        # changes 'WholeSentence' to be true for the first sentence for each sentence group\n",
    "        if last_id != row['SentenceId']:\n",
    "            last_id = row['SentenceId']\n",
    "            df.set_value(index, 'WholeSentence', True)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = add_processed_review(train)\n",
    "test = add_processed_review(test)\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 1000) \n",
    "\n",
    "train_feature = get_word_bag(vectorizer, train['ProcessedReview'].tolist())\n",
    "test_feature = get_word_bag(vectorizer, test['ProcessedReview'].tolist())\n",
    "submission(nb_model(train_feature, test_feature, train['Sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    Naive Bayesian By Hand\n",
    "</h1>\n",
    "<p> \n",
    "    Now we will try to implement our naive bayesian then compare it to scikit learns algorithm.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    We have 5 different potential reviews: 0,1,2,3,4 and will need 5 class probabilities to represent each potential outcome. We will get the frequency per word and then calculate the probability for each of the class probabilites by dividing the frequency of word with the total number of occurences that word occurs in that review. To get the probability of a phrase or sentence we multiply the probability for each word in that phrase against he probability of any document that expresses that sentiment.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will start by creating 5 probabilities for any given document that expresses that sentiment\n",
    "doc_prob_0 = float(len(train[train.Sentiment == 0]))/len(train)\n",
    "doc_prob_1 = float(len(train[train.Sentiment == 1]))/len(train)\n",
    "doc_prob_2 = float(len(train[train.Sentiment == 2]))/len(train)\n",
    "doc_prob_3 = float(len(train[train.Sentiment == 3]))/len(train)\n",
    "doc_prob_4 = float(len(train[train.Sentiment == 4]))/len(train)\n",
    "\n",
    "doc_probs = [doc_prob_0, doc_prob_1, doc_prob_2, doc_prob_3, doc_prob_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_word(df):\n",
    "    \"\"\" Takes: Test data in the form of dataframe\n",
    "        Returns: A list value of words number in different sentiment\n",
    "    \"\"\"\n",
    "    sentiment_word = np.array([0, 0, 0, 0, 0])\n",
    "    for index, row in df.iterrows():\n",
    "        sentiment = row[\"Sentiment\"]\n",
    "        sentiment_word[sentiment] += (row[\"ProcessedReview\"].count(\" \") + 1)\n",
    "    return sentiment_word\n",
    "\n",
    "def create_reference_histogram(df):\n",
    "    \"\"\"Takes: A dataset of strings\n",
    "       Returns: Dictionary w/ keys-words, value- list of values[sent=0,sent=1,sent=2,sent=3,sent=4,total] \n",
    "    \"\"\"\n",
    "    histogram = {}\n",
    "    #we will iterate through each sentence and word and increment the index of the list of values\n",
    "    for index, row in df.iterrows():\n",
    "        sentiment_value = df.Sentiment[index]\n",
    "        words = df[\"ProcessedReview\"][index].split()\n",
    "        for word in words:\n",
    "            if word in histogram:\n",
    "                histogram[word][5] +=1\n",
    "                histogram[word][sentiment_value] += 1\n",
    "            else:\n",
    "                histogram[word]= np.array([0,0,0,0,0,1])\n",
    "                histogram[word][sentiment_value] += 1\n",
    "    return histogram  \n",
    "\n",
    "def calc_sentiment_probabilities(ref_dict,sent_freq):\n",
    "    \"\"\" Takes: A sentiment value you are trying to test for\n",
    "        Returns: Dictionary w/ keys-word value-probability of that word matching the value\n",
    "        frequency = matching_value_freq/total_freq\n",
    "    \"\"\"   \n",
    "    # make a copy of ref_dict\n",
    "    final_dict = {}\n",
    "    # we will iterate through every word in the dictionary and change the values from a list to the probability we want \n",
    "    # if the value is 0 we will make probability 1 as part of smooth\n",
    "    for word in ref_dict:\n",
    "        final_dict[word] = ref_dict[word][:-1]/(sent_freq + float(ref_dict[word][5]))\n",
    "    return final_dict\n",
    "\n",
    "def smooth(lst):\n",
    "    for ele in lst:\n",
    "        if ele == 0: ele = 1\n",
    "    return lst\n",
    "\n",
    "def phrase_sentiment_prob(text, refer):\n",
    "    \"\"\" Takes: A phrase in the form of a string\n",
    "        Returns: A list of probabilities for different sentiments\n",
    "    \"\"\"\n",
    "    probs = np.asarray(doc_probs)\n",
    "    for word in text.split():\n",
    "        if word in refer:\n",
    "            probs *= smooth(refer[word])\n",
    "    return probs    \n",
    "\n",
    "def make_predictions(data, refer):\n",
    "    \"\"\" Takes: Test data in the form of dataframe\n",
    "        Returns: Submission File w. Sentiment value calculated\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    lst = []\n",
    "    for index, row in data.iterrows():\n",
    "        lst = phrase_sentiment_prob(data['ProcessedReview'][index], refer).tolist()\n",
    "        result.append(lst.index(max(lst)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_sent = sentiment_word(train)\n",
    "hist = create_reference_histogram(train)\n",
    "reference_hist = calc_sentiment_probabilities(hist,overall_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 45485, 136635, 239989, 156156,  54868])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_word(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = make_predictions(test, reference_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def reshape_list(lst):\n",
    "    \"\"\"Reshape dataset into dimension 1 arrays\"\"\"\n",
    "    for i in range(len(lst)):\n",
    "        tup_l = list(lst[i])\n",
    "        tup_l[0] = tup_l[0].reshape((1000, 1))\n",
    "        tup_l[1] = tup_l[1].reshape((1, 1))\n",
    "        lst[i] = tuple(tup_l)    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 52 / 200\n",
      "Epoch 1: 88 / 200\n",
      "Epoch 2: 89 / 200\n",
      "Epoch 3: 91 / 200\n",
      "Epoch 4: 91 / 200\n",
      "Epoch 5: 91 / 200\n",
      "Epoch 6: 93 / 200\n",
      "Epoch 7: 93 / 200\n",
      "Epoch 8: 93 / 200\n",
      "Epoch 9: 93 / 200\n",
      "Epoch 10: 93 / 200\n",
      "Epoch 11: 91 / 200\n",
      "Epoch 12: 92 / 200\n",
      "Epoch 13: 92 / 200\n",
      "Epoch 14: 92 / 200\n",
      "Epoch 15: 92 / 200\n",
      "Epoch 16: 92 / 200\n",
      "Epoch 17: 90 / 200\n",
      "Epoch 18: 90 / 200\n",
      "Epoch 19: 90 / 200\n",
      "Epoch 20: 89 / 200\n",
      "Epoch 21: 89 / 200\n",
      "Epoch 22: 89 / 200\n",
      "Epoch 23: 89 / 200\n",
      "Epoch 24: 89 / 200\n",
      "Epoch 25: 89 / 200\n",
      "Epoch 26: 89 / 200\n",
      "Epoch 27: 88 / 200\n",
      "Epoch 28: 88 / 200\n",
      "Epoch 29: 88 / 200\n"
     ]
    }
   ],
   "source": [
    "net = Network([1000, 200, 100, 80, 60, 50, 20, 5])\n",
    "val = train['Sentiment'].reshape((156060, 1))\n",
    "train_set = reshape_list(zip(train_feature, val))\n",
    "net.SGD(train_set[:1000], 30, 10, .1, test_data=train_set[1000:1200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
