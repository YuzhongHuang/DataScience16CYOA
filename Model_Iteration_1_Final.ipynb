{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Iteration 1 for Movie Sentiment Analysis </h1>\n",
    "<h2> Yuzhong Huang & Wilson Tang </h2>\n",
    "<h3> Data Description\n",
    "</h3>\n",
    "<p>\n",
    "\"There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.\"\n",
    "The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee. In their work on sentiment treebanks, Socher et al. used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Exploration Journal Debrief\n",
    "</h1>\n",
    "<p>\n",
    "We first followed the bag of words kaggle tutorial(https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words) to create a (256060, 1000) bag of words as a feature map, and applied random forest classification to fit our trainning data. Due to limitedness of memory, we were only able to perform random forest classification at tree level of 10. And we achieved score of 0.56750 in Kaggle.\n",
    "We then used scikit's naive bayesian model and explored using \n",
    "    - tfidf \n",
    "    - simple frequencies \n",
    "    - our own model(started)\n",
    "Naive Bayesian appeared to work better and we will be further working on improving it later before moving onto more techniques\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Bag of Words\n",
    "</h1>\n",
    "<p>\n",
    "To generate a bag of words, we need to do some level of data cleaning beforehand. It would be sad if some of the common meaningless stop words like \"the\", \"a\" becomes the features of our bag of word. After that, we will use scikit-learn's bag of words tool \"CountVectorizer\" to generate a vectorizer of 1000 words. By using the bag of words, we then use random forest classification of level 10 to fit our training bag of words.\n",
    "After following the tutorial, we want to incorporate our discoverings from data exploration to the bag of words. Specifically, we wanted to filter out words of high standard deviation while keeping those with high frequency.\n",
    "We first import both train and test data and drop \"Nan\" entries for basic data cleaning\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuzhong/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/yuzhong/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yuzhong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66292.00000</td>\n",
       "      <td>66292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>189206.50000</td>\n",
       "      <td>10114.909144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19136.99636</td>\n",
       "      <td>966.787807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>156061.00000</td>\n",
       "      <td>8545.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>172633.75000</td>\n",
       "      <td>9266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>189206.50000</td>\n",
       "      <td>10086.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>205779.25000</td>\n",
       "      <td>10941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>222352.00000</td>\n",
       "      <td>11855.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PhraseId    SentenceId\n",
       "count   66292.00000  66292.000000\n",
       "mean   189206.50000  10114.909144\n",
       "std     19136.99636    966.787807\n",
       "min    156061.00000   8545.000000\n",
       "25%    172633.75000   9266.000000\n",
       "50%    189206.50000  10086.000000\n",
       "75%    205779.25000  10941.000000\n",
       "max    222352.00000  11855.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"train.tsv\", sep='\\t')\n",
    "test = pd.read_csv(\"test.tsv\", sep='\\t')\n",
    "\n",
    "# drop nan data in train\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "train.describe()\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\" This function will take in a loaded review as well as a list of stop words and process it by\n",
    "    removing non letters, converting to lowercase and splitting, removing stopwords, and \n",
    "    returning a reconstructed review\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return ( \" \".join(meaningful_words ))\n",
    "\n",
    "def add_processed_review(df):\n",
    "    \"\"\" add_processed_review takes a dataframe and process the phrases within the dataframe and \n",
    "    add a new column \"ProcessedReview\".\n",
    "    \"\"\"\n",
    "    df['ProcessedReview'] = df['Phrase'].apply(review_to_words)\n",
    "    return df\n",
    "\n",
    "train = add_processed_review(train)\n",
    "test = add_processed_review(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Now that we have our training reviews tidied up, we are going to create a bag of words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears.\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 1000 most frequent words (remembering that stop words have already been removed).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 4000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(train['ProcessedReview'].tolist())\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Now that we created a bag of words from training dataset, we will use random forest classification to fit a basic model. Due to the limitedness of our computer's RAM, we set n_estimators to be 10, rather than 100 as suggested.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 10) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[\"Sentiment\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Perform the same operation for test dataset and make a submission for Kaggle \"Movie_Sentiment_Analysis_Submission.csv\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(test['ProcessedReview'].tolist())\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "def submission(predictions):\n",
    "    \"\"\"\n",
    "    Use pandas to write the comma-separated output \n",
    "    file based on the predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "            \"PhraseId\": test[\"PhraseId\"],\n",
    "            \"Sentiment\": predictions\n",
    "        })\n",
    "\n",
    "    submission.to_csv(\"Movie_Sentiment_Analysis_Submission.csv\", index=False, quoting=3)\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "submission(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By submitting the file to Kaggle, we get a score of 0.56750. From here, we then want to implement our own bag of words to generates a better bag of words based on our data explorations.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def word_df(df):\n",
    "    \"\"\"\n",
    "    Create a dataframe documenting the frequency, standard deviation and \n",
    "    mean value of each unique word appears in the given dataframe  \n",
    "    \"\"\"\n",
    "    word_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        list = review_to_words(row[\"Phrase\"]).split()\n",
    "        sentiment = row[\"Sentiment\"]\n",
    "        \n",
    "        for word in list:\n",
    "            if word in word_dict:\n",
    "                word_dict[word][0] += 1\n",
    "                word_dict[word][1].append(sentiment)\n",
    "            else:\n",
    "                word_dict[word] = [1, [sentiment]]\n",
    "    \n",
    "    word_df = pd.DataFrame(word_dict.items(), columns=[\"Word\", \"Frequency-Sentiment\"])\n",
    "    word_df['Frequency'] = word_df['Frequency-Sentiment'].apply(lambda x: x[0])\n",
    "    word_df['Sentiment'] = word_df['Frequency-Sentiment'].apply(lambda x: np.mean(x[1]))\n",
    "    word_df['Std'] = word_df['Frequency-Sentiment'].apply(lambda x: float(np.std(x[1])))\n",
    "    return word_df.drop('Frequency-Sentiment', 1).dropna().sort_values(['Frequency'], ascending=False)\n",
    "\n",
    "# generate a word dataframe for train dataset\n",
    "train_word_df = word_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jointplot of sentiment \n",
    "sns.jointplot(train_word_df['Sentiment'], train_word_df['Std']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(train_word_df['Frequency'], train_word_df['Std']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Based on the distribution of standard deviation and frequency for our training data set, we pick words that have frequency higher than 30 and standard deviation smaller than 0.8, which will gives us more accurate and meanful information in deciding the sentiment of phrases\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_review_to_words(review):\n",
    "    \"\"\" This function will take in a loaded review as well as a list of stop words and process it by\n",
    "    removing non letters, converting to lowercase and splitting, removing stopwords, and \n",
    "    returning a reconstructed review\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    words = letters_only.lower().split()\n",
    "    select = set(train_bag_word_df[\"Word\"].tolist())\n",
    "    meaningful_words = [w for w in words if w in select]\n",
    "    return ( \" \".join(meaningful_words ))\n",
    "\n",
    "def add_select_processed_review(df):\n",
    "    \"\"\" add_processed_review takes a dataframe and process the phrases within the dataframe and \n",
    "    add a new column \"ProcessedReview\".\n",
    "    \"\"\"\n",
    "    df['ProcessedReview'] = df['Phrase'].apply(lambda x: select_review_to_words(x))\n",
    "    return df\n",
    "\n",
    "train_t = add_select_processed_review(train)\n",
    "test_t = add_select_processed_review(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_t_data_features = vectorizer.fit_transform(train_t['ProcessedReview'].tolist())\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_t_data_features = train_t_data_features.toarray()\n",
    "\n",
    "forest = forest.fit( train_t_data_features, train_t[\"Sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_t_data_features = vectorizer.transform(test_t['ProcessedReview'].tolist())\n",
    "test_T_data_features = test_t_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_t_data_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use pandas to write the comma-separated output file\n",
    "submission(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We only got about 0.54 by using our own word list. I think the reason is that we only cares about frequency and standard deviation, sklean's countvector may have a more comprehensive word list\n",
    "Next, we will try applying tfidf tool over the bag of word and change words to lemma so that we focus more on the meaning of the word rather than the form of word.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    \"\"\"\n",
    "    Using TextBlob to get the root of words in a given sentence and return a list of lemmas\n",
    "    \n",
    "    Examples:\n",
    "    split_into_lemmas(\"eating meals\") == ['eat', 'meal']\n",
    "    \"\"\"\n",
    "    message = unicode(message, 'utf8').lower()\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "def lemmas_review_to_words(review):\n",
    "    \"\"\" This function will take in a loaded review as well as a list of stop words and process it by\n",
    "    removing non letters, converting to lowercase and splitting, removing stopwords, and \n",
    "    returning a reconstructed review\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    words = split_into_lemmas(letters_only)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return ( \" \".join(meaningful_words))\n",
    "\n",
    "def add_lemmas_processed_review(df):\n",
    "    \"\"\" add_processed_review takes a dataframe and process the phrases within the dataframe and \n",
    "    add a new column \"ProcessedReview\".\n",
    "    \"\"\"\n",
    "    df['ProcessedReview'] = df['Phrase'].apply(lemmas_review_to_words)\n",
    "    return df\n",
    "\n",
    "train_l = add_lemmas_processed_review(train)\n",
    "test_l = add_lemmas_processed_review(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_l_data_features = vectorizer.fit_transform(train_l['ProcessedReview'].tolist())\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_l_tfidf = TfidfTransformer().fit_transform(train_l_data_features)\n",
    "train_l_tfidf = train_l_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 10) \n",
    "forest = forest.fit(train_l_tfidf, train_l[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_l_data_features = vectorizer.transform(test_l['ProcessedReview'].tolist())\n",
    "test_l_tfidf = TfidfTransformer().fit_transform(test_l_data_features)\n",
    "test_l_tfidf = test_l_tfidf.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_l_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use pandas to write the comma-separated output file\n",
    "submission(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "We got about 0.56533 by using tfidf, which is slightly lower than what we got before\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> \n",
    "Next we will try out Naive Bayesian from scikit learn\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(train_l_data_features, train_l[\"Sentiment\"])\n",
    "result = nb.predict(test_l_data_features)\n",
    "submission(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "We got about 0.55569 by using navie bayes with tfidf, which is lower than what we got before. But if we just count the frequency of the words for bag of words, we get 0.57274 in Kaggle, which is better than that combination with random forest</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The difference in our scores between naive bayesian and tfidf could be attributed to the naive bayesian potentially taking into account the std of our data set with bayesian calculating different probabilities for each potential result takes into account the variability of the word better than tfidf's frequency\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>\n",
    "Next step is to incorporate our findings with length and relationship between sentences and phrases to our current best modeling: navie bayes with frequencet of bag of words. Specifically, we are going to have seperate data for whole sentences and single phrases. We will first model whole sentences and then apply length and sentences's sentiment\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_whole_sentence(df):\n",
    "    \"\"\"\n",
    "    Generates a boolean 'WholeSentence' column for given dataset\n",
    "    to indicate whether the phrase is a whole sentence or not\n",
    "    \"\"\"\n",
    "    df['WholeSentence'] = False\n",
    "    # initialize a last_id for comparison\n",
    "    last_id = 0\n",
    "    # loop throught the dataframe to set the first sentence for each sentence group's 'WholeSentence' to be True\n",
    "    for index, row in df.iterrows():\n",
    "        # changes 'WholeSentence' to be true for the first sentence for each sentence group\n",
    "        if last_id != row['SentenceId']:\n",
    "            last_id = row['SentenceId']\n",
    "            df.set_value(index, 'WholeSentence', True)\n",
    "    return df \n",
    "\n",
    "train = add_whole_sentence(train)\n",
    "test = add_whole_sentence(test)\n",
    "train_sentence =  train[train['WholeSentence'] == True]\n",
    "test_sentence =  test[test['WholeSentence'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_s_data_features = vectorizer.fit_transform(train_sentence['ProcessedReview'].tolist())\n",
    "train_s_data_features = train_s_data_features.toarray()\n",
    "test_s_data_features = vectorizer.transform(test_sentence['ProcessedReview'].tolist())\n",
    "test_s_data_features = test_s_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now we will try to fit the data based on the overall sentence sentiment \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(train_s_data_features, train_sentence[\"Sentiment\"])\n",
    "result = nb.predict(test_s_data_features)\n",
    "j = 0\n",
    "for index, row in test_sentence.iterrows():\n",
    "    test_sentence.set_value(index, 'SentenceSentiment', result[j])\n",
    "    j += 1\n",
    "test['SentenceSentiment'] = 2\n",
    "for index, row in test_sentence.iterrows():\n",
    "    test.set_value(index, 'SentenceSentiment', test_sentence['SentenceSentiment'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate_sentence_sentiment(df):\n",
    "    \"\"\"\n",
    "    Generates a boolean 'WholeSentence' column for given dataset\n",
    "    to indicate whether the phrase is a whole sentence or not\n",
    "    \"\"\"\n",
    "    last_id = 0\n",
    "    last_value = 2\n",
    "    # loop throught the dataframe to set the first sentence for each sentence group's 'WholeSentence' to be True\n",
    "    for index, row in df.iterrows():\n",
    "        # changes 'WholeSentence' to be true for the first sentence for each sentence group\n",
    "        if last_id != row['SentenceId']:\n",
    "            last_id = row['SentenceId']\n",
    "            last_value = row['SentenceSentiment']\n",
    "        else:\n",
    "            df.set_value(index, 'SentenceSentiment', last_value)\n",
    "    return df \n",
    "test = propagate_sentence_sentiment(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission(test['SentenceSentiment'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Overall we had the best results with naive bayesian, then bag of word tfidf, then using naive bayesian with sentence propogation. We think that when given the right structured data scikit learns multinomial naive bayesian model is able to learn better (tfidf vs simple frequency) and did some exploration into trying to improve the naive bayesian model\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In addition we have started trying to code a naive bayesian model from scatch(model_iteration_1_1) to see if we can gain more insights after comparing the results to scikits learn.(we are interested in the nuts and bolts of it). After doing some more improvements on naive bayesian we will try to set up some deep learning or neural network algorithms (the other machine learning methods we want to learn about)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
